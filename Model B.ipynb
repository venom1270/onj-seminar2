{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A ALL questions - CoreNLP + TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "81\n",
      "81\n",
      "70\n",
      "76\n",
      "67\n",
      "69\n",
      "71\n",
      "69\n",
      "62\n",
      "65\n",
      "68\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/dataset - fixed.csv\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "first = True\n",
    "questions_all = []\n",
    "answers_all = []\n",
    "grades_all = []\n",
    "texts_all = []\n",
    "for line in f:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "    s = line.split(\";\")\n",
    "    questions_all.append(s[10])\n",
    "    answers_all.append(s[11])\n",
    "    grades_all.append(s[14])\n",
    "    texts_all.append(s[15])\n",
    "    if s[15] == '1':\n",
    "        print(line)\n",
    "#test_answers = [answers_all[i] for i in range(len(questions_all)) if questions_all[i] == questions[0]]\n",
    "#test_grades = [float(grades_all[i].replace(\",\", \".\")) for i in range(len(questions_all)) if questions_all[i] == questions[0]]\n",
    "\n",
    "## DATA[question_number] -> tuples of (queston, grade, answer, text)\n",
    "old = questions_all[0]\n",
    "DATA = []\n",
    "data_tmp = []\n",
    "for i in range(len(questions_all)):\n",
    "    if questions_all[i] == old:\n",
    "        triple = (questions_all[i], grades_all[i], answers_all[i], texts_all[i])\n",
    "        data_tmp.append(triple)\n",
    "    else:\n",
    "        old = questions_all[i]\n",
    "        DATA.append(data_tmp)\n",
    "        data_tmp = []\n",
    "        data_tmp.append((questions_all[i], grades_all[i], answers_all[i], texts_all[i]))\n",
    "DATA.append(data_tmp)\n",
    "        \n",
    "print(len(DATA))\n",
    "for i in DATA:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8 #0.8 train data, 0.2 test data FOR EACH QUESTION\n",
    "\n",
    "DATA_train = []\n",
    "DATA_test = []\n",
    "\n",
    "for i in DATA:\n",
    "    split = int(len(i) * ratio)\n",
    "    DATA_train.append(i[:split])\n",
    "    DATA_test.append(i[split:])\n",
    "\n",
    "#Test and verification\n",
    "#for i in range(len(DATA)):\n",
    "#    print(len(DATA[i]))\n",
    "#    print(len(DATA_train[i]) + len(DATA_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://localhost:9000/?properties={\"annotators\": \"openie,coref\", \"outputFormat\": \"json\", \"openie.resolve_coref\": \"true\"}'\n",
    "data = \"Sarah jumped over the table. She got hurt.\"\n",
    "response = requests.post(url, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Sarah  | Relation: jumped over  | Object: table\n",
      "Subject: Sarah  | Relation: got  | Object: hurt\n"
     ]
    }
   ],
   "source": [
    "for s in response.json()[\"sentences\"]:\n",
    "    for i in s[\"openie\"]:\n",
    "        print(\"Subject:\", i[\"subject\"], \" | Relation:\", i[\"relation\"], \" | Object:\", i[\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openie_extract(text, resolve_coref=True):\n",
    "    import requests\n",
    "    if resolve_coref:\n",
    "        url = 'http://localhost:9000/?properties={\"annotators\": \"tokenize,ssplit,pos,lemma,openie,coref\", \"outputFormat\": \"json\", \"openie.resolve_coref\": \"true\", \"openie.triple.strict\": \"true\"}'\n",
    "    else:\n",
    "        url = 'http://localhost:9000/?properties={\"annotators\": \"tokenize,ssplit,pos,lemma,openie,coref\", \"outputFormat\": \"json\", \"openie.resolve_coref\": \"false\", \"openie.triple.strict\": \"true\"}'\n",
    "    data = text\n",
    "    response = requests.post(url, data=data)\n",
    "    response.encoding = \"utf-8\"\n",
    "    triples = []\n",
    "    for s in response.json()[\"sentences\"]:\n",
    "        for i in s[\"openie\"]:\n",
    "            #print(\"Subject:\", i[\"subject\"], \" | Relation:\", i[\"relation\"], \" | Object:\", i[\"object\"])\n",
    "            #triples.append((lemmatize([i[\"subject\"]])[0], lemmatize([i[\"relation\"]])[0], lemmatize([i[\"object\"]])[0]))\n",
    "            triples.append((i[\"subject\"], i[\"relation\"], i[\"object\"]))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## DATA[question_number] -> tuples of (queston, grade, answer, text)\n",
    "\n",
    "BASE_TRIPLES = []\n",
    "for i in DATA_train:\n",
    "    data = i[0][3] + \" \"#answers[i] + \" \" + texts[i]\n",
    "    for j in i: #loop through all tuples\n",
    "        data += j[2] + \" \"\n",
    "    BASE_TRIPLES.append(openie_extract(data.encode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def getTokens(text):\n",
    "    lowered = text.lower()\n",
    "    table = text.maketrans({key: None for key in string.punctuation})\n",
    "    lowered = lowered.translate(table)\n",
    "    return nltk.word_tokenize(lowered)\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = getTokens(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "    #pos_tag = nltk.pos_tag(lemmas)\n",
    "    #print(pos_tag)\n",
    "    #return \" \".join([pt[0] for pt in pos_tag if pt[1] == \"NN\" or pt[1][0:2] == \"VB\" or pt[1] == \"JJ\"])\n",
    "\n",
    "\n",
    "#pre_answers = [preprocess(ans) for ans in answers]\n",
    "#pre_texts = [preprocess(tex) for tex in texts]\n",
    "\n",
    "#pre_answers[question_number] -> array of preprocessed answers\n",
    "#pre_text[question_number] -> preprocessed text for that question\n",
    "\n",
    "## DATA[question_number] -> tuples of (queston, grade, answer, text)\n",
    "\n",
    "pre_answers = []\n",
    "pre_texts = []\n",
    "for i in DATA_train:\n",
    "    pre_answers.append([preprocess(ans[2]) for ans in i])\n",
    "    pre_texts.append(preprocess(i[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA -> ALL data from csv, DATA[question_number] -> triples of (question, grade, answer, text)\n",
    "\n",
    "answers -> answers for model A (one for each question)\n",
    "\n",
    "texts -> texts for model A (one for each question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  114 / 176\n"
     ]
    }
   ],
   "source": [
    "#weights = vect.transform(test_answers)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "correct = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "all_count = 0\n",
    "\n",
    "#for F1 scoring\n",
    "true_grades = []\n",
    "predicted_grades = []\n",
    "\n",
    "## DATA[question_number] -> triples of (queston, grade, answer, text)\n",
    "for d in range(len(DATA_test)):\n",
    "    test_answers = [a[2] for a in DATA_test[d]]\n",
    "    test_grades = [float(a[1]) for a in DATA_test[d]]\n",
    "    \n",
    "    true_grades += test_grades\n",
    "\n",
    "    vect = TfidfVectorizer() # parameters for tokenization, stopwords can be passed\n",
    "    #tfidf = vect.fit_transform([pre_answers[d], pre_texts[d]])\n",
    "    tfidf = vect.fit_transform([pre_texts[d]] + pre_answers[d])\n",
    "    \n",
    "    weights = vect.transform([preprocess(ta) for ta in test_answers])\n",
    "    predict = tfidf * weights.T\n",
    "\n",
    "    #Shiranna feels excited and scared as the shuttle is taking off and it even affects her heart-rate and her temperature.\n",
    "    #test_answers = [\"Excited, scared. It affected her heart rate and temp\"]\n",
    "\n",
    "    \n",
    "    for i in range(len(test_answers)):\n",
    "        all_count += 1 # only for statistics at the end\n",
    "        \n",
    "        test_answer = test_answers[i]\n",
    "        triples = openie_extract(test_answer.encode(\"utf8\"))\n",
    "\n",
    "        p = 0\n",
    "        p_tfidf = 0\n",
    "        p_triples = 0\n",
    "\n",
    "        #if len(triples) < 1:\n",
    "        #prediction = max(predict[0,i], predict[1,i])\n",
    "        prediction = max(predict[:,i])\n",
    "        if prediction > 0.35: #0.5\n",
    "            p_tfidf = 1\n",
    "        elif prediction > 0.2: #0.3\n",
    "            p_tfidf = 0.5\n",
    "        else:\n",
    "            p_tfidf = 0\n",
    "        #else:\n",
    "        for bt in BASE_TRIPLES[d]:\n",
    "            for t in triples:\n",
    "                if t[0] == bt[0] or t[1] == bt[1] or t[2] == bt[2]:\n",
    "                #if (t[0] == bt[0] and t[1] == bt[1]) or (t[0] == bt[0] and t[2] == bt[2]) or (t[1] == bt[1] and t[2] == bt[2]):\n",
    "                    p_triples += 0.5\n",
    "        p_triples = p_triples*4 / max(len(triples), 1)\n",
    "        prediction = 0\n",
    "        if p_triples >= 1:\n",
    "            p_triples = 1\n",
    "        elif p_triples >= 0.5:\n",
    "            p_triples = 0.5\n",
    "        else:\n",
    "            p_triples = 0\n",
    "\n",
    "        p = (p_triples + p_tfidf) / 2\n",
    "        if p > 0 and p < 1 and p != 0.5:\n",
    "            p = 0.5\n",
    "            \n",
    "        #p = p_tfidf\n",
    "        \n",
    "        predicted_grades.append(p)\n",
    "        #print(\"Predicted:\", p, \" Real:\", test_grades[i], \" --- \", len(triples))\n",
    "        \n",
    "        if p == test_grades[i]:\n",
    "            correct += 1\n",
    "            if p == 1 or p == 0.5:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            #print(len(triples))\n",
    "            if p == 1 or p == 0.5:\n",
    "                FP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    \n",
    "print(\"Correct: \", correct, \"/\", all_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[texts[0] + answers[0]] 0.3 0.2 -> 38/81 correct\n",
    "\n",
    "[texts[0], answers[0]]  0.3 0.2 -> 42/81 correct \n",
    "\n",
    "CoreNLP openie+coref /w TFIDF -> 32/81 correct\n",
    "\n",
    "CoreNLP openie /w TFIDF ->  40/81 correct\n",
    "\n",
    "CoreNLP openie relative scoring /w TFIDF -> 36/81 correct\n",
    "\n",
    "CoreNLP openie+coref relative scoring /w TFIDF -> 33/81 correct\n",
    "\n",
    "CoreNLP (not strict) openie+coref relative scoring AND TFIDF | AVG -> 43/81 correct\n",
    "\n",
    "CoreNLP openie+coref relative scoring AND TFIDF | AVG -> 40/81 correct\n",
    "\n",
    "CoreNLP openie+coref(with and without) relative scoring AND TFIDF | AVG -> 41/81 correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vsa vprašanja (mikro, makro):  0.6477272727272727 0.41571002719682665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "tg = [i*2 for i in true_grades]\n",
    "pg = [i*2 for i in predicted_grades]\n",
    "#print(tg)\n",
    "#print(pg)\n",
    "print(\"Vsa vprašanja (mikro, makro): \", f1_score(tg,pg,average=\"micro\"), f1_score(tg,pg,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score: openIE + coref relative scoring averaged with TFIDF\n",
    "Correct:  114 / 176\n",
    "\n",
    "Vsa vprašanja (mikro, makro):  0.6477272727272727 0.41571002719682665\n",
    "\n",
    "### F1 score: openIE + coref relative scoring WITHOUT TFIDF\n",
    "Correct:  116 / 176\n",
    "\n",
    "Vsa vprašanja (mikro, makro):  0.6590909090909091 0.35599276345545\n",
    "\n",
    "### F1 score: only TFIDF\n",
    "Correct:  129 / 176\n",
    "\n",
    "Vsa vprašanja (mikro, makro):  0.7329545454545454 0.4020678828560602"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
