{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A - CoreNLP + TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java -mx3g -cp \"C:\\Users\\zigsi\\Desktop\\CoreNLP\\stanford-corenlp-full-2018-10-05/*\" edu.stanford.nlp.naturalli.OpenIE text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just the thought of leaving Earth both thrilled and terrified her. Her heart stopped as the trailer-sized shuttle moved forward on the track without making a sound. She took in a deep breath. she said, trying to be brave\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/Weightless_dataset_train_A.csv\", \"r\", encoding=\"utf-8\")\n",
    "first = True\n",
    "questions = []\n",
    "answers = []\n",
    "grades = []\n",
    "texts = []\n",
    "for line in f:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "    s = line.split(\";\")\n",
    "    questions.append(s[10])\n",
    "    answers.append(s[11])\n",
    "    grades.append(s[14])\n",
    "    texts.append(s[15])\n",
    "    \n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data (first question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 81, 70, 76, 67, 69, 71, 69, 62, 65, 68, 71]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/dataset.csv\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "first = True\n",
    "questions_all = []\n",
    "answers_all = []\n",
    "grades_all = []\n",
    "texts_all = []\n",
    "for line in f:\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "    s = line.split(\";\")\n",
    "    questions_all.append(s[10])\n",
    "    answers_all.append(s[11])\n",
    "    grades_all.append(s[14])\n",
    "    texts_all.append(s[15])\n",
    "test_answers = [answers_all[i] for i in range(len(questions_all)) if questions_all[i] == questions[0]]\n",
    "test_grades = [float(grades_all[i].replace(\",\", \".\")) for i in range(len(questions_all)) if questions_all[i] == questions[0]]\n",
    "\n",
    "\n",
    "test_answers_tfidf = []\n",
    "test_grades_tfidf = []\n",
    "for q in range(len(questions)):\n",
    "    test_answers_tfidf.append([answers_all[i] for i in range(len(questions_all)) if questions_all[i] == questions[q]])\n",
    "    test_grades_tfidf.append([float(grades_all[i].replace(\",\", \".\")) for i in range(len(questions_all)) if questions_all[i] == questions[q]])\n",
    "print([len(t) for t in test_answers_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://localhost:9000/?properties={\"annotators\": \"openie\", \"outputFormat\": \"json\"}'\n",
    "data = \"Fox jumped over the table. She got hurt.\"\n",
    "response = requests.post(url, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Fox  | Relation: jumped over  | Object: table\n",
      "Subject: She  | Relation: got  | Object: hurt\n"
     ]
    }
   ],
   "source": [
    "for s in response.json()[\"sentences\"]:\n",
    "    for i in s[\"openie\"]:\n",
    "        print(\"Subject:\", i[\"subject\"], \" | Relation:\", i[\"relation\"], \" | Object:\", i[\"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openie_extract(text):\n",
    "    import requests\n",
    "    url = 'http://localhost:9000/?properties={\"annotators\": \"tokenize,ssplit,pos,lemma,openie\", \"outputFormat\": \"json\"}'\n",
    "    data = text\n",
    "    response = requests.post(url, data=data)\n",
    "    response.encoding = \"utf-8\"\n",
    "    triples = []\n",
    "    for s in response.json()[\"sentences\"]:\n",
    "        for i in s[\"openie\"]:\n",
    "            #print(\"Subject:\", i[\"subject\"], \" | Relation:\", i[\"relation\"], \" | Object:\", i[\"object\"])\n",
    "            #triples.append((lemmatize([i[\"subject\"]])[0], lemmatize([i[\"relation\"]])[0], lemmatize([i[\"object\"]])[0]))\n",
    "            triples.append((i[\"subject\"], i[\"relation\"], i[\"object\"]))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shiranna', 'feels', 'excited'),\n",
       " ('it', 'affects', 'her heart-rate'),\n",
       " ('it', 'even affects', 'her heart-rate'),\n",
       " ('Her heart', 'stopped', 'shuttle moved on track'),\n",
       " ('shuttle', 'moved on', 'track'),\n",
       " ('shuttle', 'making', 'sound'),\n",
       " ('Her heart', 'stopped', 'trailer-sized shuttle moved'),\n",
       " ('trailer-sized shuttle', 'moved forward on', 'track'),\n",
       " ('trailer-sized shuttle', 'making', 'sound'),\n",
       " ('Her heart', 'stopped', 'trailer-sized shuttle moved forward on track'),\n",
       " ('trailer-sized shuttle', 'moved on', 'track'),\n",
       " ('Her heart', 'stopped', 'shuttle moved forward'),\n",
       " ('Her heart', 'stopped', 'shuttle moved'),\n",
       " ('Her heart', 'stopped', 'trailer-sized shuttle moved forward'),\n",
       " ('shuttle', 'moved forward on', 'track'),\n",
       " ('Her heart', 'stopped', 'shuttle moved forward on track'),\n",
       " ('Her heart', 'stopped', 'trailer-sized shuttle moved on track'),\n",
       " ('She', 'took in', 'deep breath'),\n",
       " ('She', 'took in', 'breath'),\n",
       " ('she', 'said', 'trying'),\n",
       " ('she', 'trying', 'brave')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = answers[0] + \" \" + texts[0]\n",
    "base_triples = openie_extract(data)\n",
    "base_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def getTokens(text):\n",
    "    lowered = text.lower()\n",
    "    table = text.maketrans({key: None for key in string.punctuation})\n",
    "    lowered = lowered.translate(table)\n",
    "    return nltk.word_tokenize(lowered)\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = getTokens(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "    #pos_tag = nltk.pos_tag(lemmas)\n",
    "    #print(pos_tag)\n",
    "    #return \" \".join([pt[0] for pt in pos_tag if pt[1] == \"NN\" or pt[1][0:2] == \"VB\" or pt[1] == \"JJ\"])\n",
    "\n",
    "\n",
    "pre_answer = preprocess(answers[0])\n",
    "pre_text = preprocess(texts[0])\n",
    "#print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1  Real: 0.5\n",
      "5\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.0\n",
      "1\n",
      "Predicted: 1  Real: 0.5\n",
      "0\n",
      "Predicted: 0.5  Real: 1.0\n",
      "0\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 1  Real: 0.5\n",
      "2\n",
      "Predicted: 1  Real: 0.5\n",
      "0\n",
      "Predicted: 0.5  Real: 1.0\n",
      "0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "13\n",
      "Predicted: 1  Real: 0.5\n",
      "0\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 1  Real: 0.5\n",
      "2\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "2\n",
      "Predicted: 1  Real: 0.5\n",
      "6\n",
      "Predicted: 0.5  Real: 1.0\n",
      "1\n",
      "Predicted: 0.5  Real: 1.0\n",
      "1\n",
      "Predicted: 0  Real: 0.5\n",
      "0\n",
      "Predicted: 1  Real: 0.5\n",
      "2\n",
      "Predicted: 1  Real: 0.5\n",
      "8\n",
      "Predicted: 1  Real: 0.0\n",
      "0\n",
      "Predicted: 1  Real: 0.5\n",
      "1\n",
      "Predicted: 0.5  Real: 0.0\n",
      "0\n",
      "Predicted: 0  Real: 0.5\n",
      "0\n",
      "Predicted: 0  Real: 0.5\n",
      "0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "2\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "3\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 0  Real: 0.5\n",
      "0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 0.5  Real: 1.0\n",
      "1\n",
      "Predicted: 1  Real: 0.5\n",
      "0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "1\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "5\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "7\n",
      "Predicted: 1  Real: 0.5\n",
      "2\n",
      "Predicted: 0.5  Real: 1.0\n",
      "1\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 0.5\n",
      "1\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 0.5  Real: 1.0\n",
      "1\n",
      "Predicted: 1  Real: 1.0\n",
      "Predicted: 0  Real: 0.5\n",
      "0\n",
      "Predicted: 0  Real: 0.5\n",
      "0\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 0  Real: 0.0\n",
      "Predicted: 0  Real: 0.5\n",
      "0\n",
      "Predicted: 0.5  Real: 1.0\n",
      "0\n",
      "Predicted: 0.5  Real: 0.5\n",
      "Predicted: 0.5  Real: 1.0\n",
      "0\n",
      "Predicted: 0.5  Real: 1.0\n",
      "0\n",
      "Predicted: 0.5  Real: 0.0\n",
      "0\n",
      "Correct:  40 / 81\n"
     ]
    }
   ],
   "source": [
    "#weights = vect.transform(test_answers)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer() # parameters for tokenization, stopwords can be passed\n",
    "tfidf = vect.fit_transform([pre_answer, pre_text])\n",
    "#print(\"Cosine similarity between the documents: \\n{}\".format(cosine))\n",
    "weights = vect.transform([preprocess(ta) for ta in test_answers_tfidf[0]])\n",
    "predict = tfidf * weights.T\n",
    "\n",
    "\n",
    "correct = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for i in range(len(test_answers)):\n",
    "    test_answer = test_answers[i]\n",
    "    triples = openie_extract(test_answer.encode(\"utf8\"))\n",
    "    \n",
    "    p = 0\n",
    "    \n",
    "    if len(triples) < 1:\n",
    "        prediction = max(predict[0,i], predict[1,i])\n",
    "        if prediction > 0.35: #0.5\n",
    "            p = 1\n",
    "        elif prediction > 0.2: #0.3\n",
    "            p = 0.5\n",
    "        else:\n",
    "            p = 0\n",
    "    else:\n",
    "        for bt in base_triples:\n",
    "            for t in triples:\n",
    "                if t[0] == bt[0] or t[1] == bt[1] or t[2] == bt[2]:\n",
    "                    p += 0.5\n",
    "        prediction = 0\n",
    "        if p >= 1:\n",
    "            p = 1\n",
    "        elif p >= 0.5:\n",
    "            p = 0.5\n",
    "        else:\n",
    "            p = 0\n",
    "    \n",
    "    print(\"Predicted:\", p, \" Real:\", test_grades[i])\n",
    "    \n",
    "    if p == test_grades[i]:\n",
    "        correct += 1\n",
    "        if p == 1 or p == 0.5:\n",
    "            TP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    else:\n",
    "        print(len(triples))\n",
    "        if p == 1 or p == 0.5:\n",
    "            FP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "    \n",
    "print(\"Correct: \", correct, \"/\", len(test_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[texts[0] + answers[0]] 0.3 0.2 -> 38/81 correct\n",
    "\n",
    "[texts[0], answers[0]]  0.3 0.2 -> 42/81 correct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = TP + TN\n",
    "A = T + FP + FN\n",
    "P = TP/(TP+FP)\n",
    "R = TP/(TP+FN)\n",
    "F1 = 2*P*R/(P+R)\n",
    "print(\"Classification Accuracy: \", T, \"/\", A, \" = \", T/A)\n",
    "print(\"Precision: \", TP, \"/\", TP+FP, \" = \", P)\n",
    "print(\"Recall: \", TP, \"/\", TP+FN, \" = \", TP/(TP+FN))\n",
    "print(\"F1: \", 2*P*R, \"/\", P+R, \" = \", F1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "** WITHOUT p == 0.5 **\n",
    "Classification Accuracy:  42 / 81  =  0.5185185185185185\n",
    "Precision:  30 / 54  =  0.5555555555555556\n",
    "Recall:  30 / 45  =  0.6666666666666666\n",
    "F1:  0.7407407407407407 / 1.2222222222222223  =  0.606060606060606\n",
    "\n",
    "** WITH p == 0.5 **\n",
    "Classification Accuracy:  42 / 81  =  0.5185185185185185\n",
    "Precision:  41 / 73  =  0.5616438356164384\n",
    "Recall:  41 / 48  =  0.8541666666666666\n",
    "F1:  0.9594748858447488 / 1.415810502283105  =  0.6776859504132232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
